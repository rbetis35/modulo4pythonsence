{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b34ef1",
   "metadata": {},
   "source": [
    "# Proyecto Integrador ‚Äî M√≥dulo 4: Preparaci√≥n de Datos (NumPy + Pandas)\n",
    "## Contexto\n",
    "Este proyecto simula el flujo de trabajo del equipo de anal√≠tica de datos de una empresa e-commerce, preparando un dataset a partir de m√∫ltiples fuentes y resolviendo problemas comunes de calidad de datos (nulos, duplicados, formatos inconsistentes y outliers).\n",
    "\n",
    "## Objetivo\n",
    "Desarrollar un proceso reproducible para:\n",
    "\n",
    "Generar datos ficticios con NumPy.\n",
    "Explorar y transformar datos con Pandas.\n",
    "Integrar fuentes (CSV, Excel y web).\n",
    "Limpiar nulos y tratar outliers con t√©cnicas estad√≠sticas.\n",
    "Aplicar data wrangling, agrupamientos y pivoteo.\n",
    "Exportar el dataset final a CSV y Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38a2b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proyecto Integrador - Modulo 4: Preparaci√≥n de Datos\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8c5ad9",
   "metadata": {},
   "source": [
    "\n",
    "## Lecci√≥n 1 ‚Äî Generaci√≥n de datos con NumPy\n",
    "¬øPor qu√© NumPy es eficiente para datos num√©ricos?\n",
    "NumPy es eficiente porque trabaja con arrays homog√©neos en memoria continua y aplica operaciones vectorizadas (sin bucles de Python), lo que permite:\n",
    "\n",
    "Mayor velocidad en c√°lculos (media, suma, conteos, etc.).\n",
    "Menor consumo de memoria comparado con listas de Python.\n",
    "Facilidad para generar datos sint√©ticos con np.random.\n",
    "En esta etapa se generan clientes ficticios y se introducen intencionalmente valores problem√°ticos (un nulo y un outlier) para aplicar limpieza en etapas posteriores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f45b752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monto promedio (sin contar nulos): 7588.444678741504\n",
      "‚úÖ Lecci√≥n 1: Datos generados.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- LECCI√ìN 1: GENERACI√ìN DE DATOS CON NUMPY ---\n",
    "# Vamos a generar 10 clientes extra para simular que llegan datos nuevos\n",
    "\n",
    "np.random.seed(42)\n",
    "ids_extra = np.arange(101, 111) # IDs del 101 al 110\n",
    "edades_extra = np.random.randint(18, 65, size=10)\n",
    "compras_extra = np.random.randint(1, 15, size=10)\n",
    "montos_extra = np.random.uniform(500, 5000, size=10)\n",
    "\n",
    "# Introducimos un nulo y un outlier para cumplir con la limpieza posterior\n",
    "montos_extra[0] = np.nan\n",
    "montos_extra[1] = 50000 # Outlier exagerado\n",
    "\n",
    "# Operaciones b√°sicas\n",
    "print(f\"Monto promedio (sin contar nulos): {np.nanmean(montos_extra)}\")\n",
    "\n",
    "# Guardamos\n",
    "datos_extra = np.column_stack((ids_extra, edades_extra, compras_extra, montos_extra))\n",
    "np.save('datos_extra.npy', datos_extra)\n",
    "print(\"‚úÖ Lecci√≥n 1: Datos generados.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b487c",
   "metadata": {},
   "source": [
    "## Lecci√≥n 2 ‚Äî Exploraci√≥n inicial con Pandas\n",
    "En esta etapa se convierte el array de NumPy en un DataFrame, lo que permite:\n",
    "\n",
    "Inspecci√≥n r√°pida con head(), describe() y filtros.\n",
    "Manipulaci√≥n tabular (columnas, tipos, transformaciones).\n",
    "Exportaci√≥n a formatos comunes para an√°lisis (.csv).\n",
    "Se genera un CSV preliminar para usarlo como insumo en la integraci√≥n de fuentes de la Lecci√≥n 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a263217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID  Edad  Total_Compras   Monto_Total           Nombre    Ciudad\n",
      "0  101.0  56.0           11.0           NaN  Cliente_Extra_1  Santiago\n",
      "1  102.0  46.0            8.0  50000.000000  Cliente_Extra_2  Santiago\n",
      "2  103.0  32.0            5.0    503.504446  Cliente_Extra_3  Santiago\n",
      "3  104.0  60.0            4.0   4964.952017  Cliente_Extra_4  Santiago\n",
      "4  105.0  25.0            8.0   3278.666793  Cliente_Extra_5  Santiago\n",
      "‚úÖ Lecci√≥n 2: DataFrame creado y exportado.\n"
     ]
    }
   ],
   "source": [
    "# --- LECCI√ìN 2: EXPLORACI√ìN CON PANDAS ---\n",
    "\n",
    "# Convertimos los datos de NumPy a DataFrame\n",
    "df_extra = pd.DataFrame(datos_extra, columns=['ID', 'Edad', 'Total_Compras', 'Monto_Total'])\n",
    "\n",
    "# Agregamos nombres ficticios para que coincida con tus otros archivos\n",
    "df_extra['Nombre'] = ['Cliente_Extra_' + str(i) for i in range(1, 11)]\n",
    "df_extra['Ciudad'] = 'Santiago' # Ciudad por defecto\n",
    "\n",
    "# Exploraci√≥n\n",
    "print(df_extra.head())\n",
    "df_extra.to_csv('clientes_extra.csv', index=False)\n",
    "print(\"‚úÖ Lecci√≥n 2: DataFrame creado y exportado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f36ec",
   "metadata": {},
   "source": [
    "## Lecci√≥n 3 ‚Äî Obtenci√≥n de datos desde m√∫ltiples fuentes (CSV, Excel y web)\n",
    "En esta etapa se integran tres fuentes:\n",
    "\n",
    "clientes_ecommerce.csv\n",
    "clientes_ecommerce.xlsx\n",
    "clientes_extra.csv (proveniente de los datos generados con NumPy)\n",
    "Desaf√≠o encontrado: al intentar extraer una tabla web con pd.read_html(), la web respondi√≥ con un error HTTP 403: Forbidden.\n",
    "Para evitar que el pipeline falle, se implement√≥ un manejo de errores (try/except) y se continu√≥ con las fuentes locales. Esto es importante en escenarios reales, donde las fuentes web pueden bloquear bots o cambiar su estructura.\n",
    "\n",
    "El resultado de esta etapa es un dataset consolidado con 30 registros (antes de limpieza), listo para tratar nulos, duplicados y outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dd502fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No se pudo leer la web, pero continuaremos con los archivos locales. Error: HTTP Error 403: Forbidden\n",
      "\n",
      "üìä Total de registros unificados: 30\n",
      "\n",
      "--- Primeras filas del dataset consolidado ---\n",
      "       ID            Nombre  Edad    Ciudad  Total_Compras  Monto_Total\n",
      "25  106.0   Cliente_Extra_6  38.0  Santiago            8.0  3252.439222\n",
      "26  107.0   Cliente_Extra_7  56.0  Santiago            3.0   531.798373\n",
      "27  108.0   Cliente_Extra_8  36.0  Santiago            6.0   603.780913\n",
      "28  109.0   Cliente_Extra_9  40.0  Santiago            5.0  2861.485971\n",
      "29  110.0  Cliente_Extra_10  28.0  Santiago            2.0  2299.374373\n"
     ]
    }
   ],
   "source": [
    "# --- LECCI√ìN 3: OBTENCI√ìN DE DATOS DESDE ARCHIVOS ---\n",
    "\n",
    "# 1. Leer el CSV y el Excel que ya tienes en la carpeta\n",
    "df_csv = pd.read_csv('clientes_ecommerce.csv')\n",
    "df_excel = pd.read_excel('clientes_ecommerce.xlsx')\n",
    "\n",
    "# 2. Leer datos desde una p√°gina web (Requerimiento t√©cnico)\n",
    "# Usaremos una tabla de Wikipedia con c√≥digos de pa√≠ses o similar, solo para demostrar la t√©cnica\n",
    "try:\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes\"\n",
    "    tablas = pd.read_html(url)\n",
    "    df_web = tablas[0].iloc[:10, :2] # Tomamos solo un pedacito para el ejemplo\n",
    "    print(\"‚úÖ Datos web extra√≠dos con √©xito.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è No se pudo leer la web, pero continuaremos con los archivos locales. Error: {e}\")\n",
    "\n",
    "# 3. Unificar las fuentes locales (CSV, Excel y los datos de NumPy que creamos)\n",
    "# Primero cargamos el que creamos en la Lecci√≥n 2\n",
    "df_extra = pd.read_csv('clientes_extra.csv')\n",
    "\n",
    "# Concatenamos todos los DataFrames de clientes\n",
    "df_consolidado = pd.concat([df_csv, df_excel, df_extra], ignore_index=True)\n",
    "\n",
    "# 4. Guardar el DataFrame consolidado\n",
    "df_consolidado.to_csv('clientes_consolidado.csv', index=False)\n",
    "\n",
    "print(f\"\\nüìä Total de registros unificados: {len(df_consolidado)}\")\n",
    "print(\"\\n--- Primeras filas del dataset consolidado ---\")\n",
    "print(df_consolidado.tail()) # Usamos tail para ver los que agregamos al final\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76bfc59",
   "metadata": {},
   "source": [
    "## Lecci√≥n 4 ‚Äî Valores perdidos (nulos) y outliers\n",
    "Valores nulos\n",
    "Se identificaron nulos en:\n",
    "\n",
    "Edad (2 registros)\n",
    "Monto_Total (1 registro)\n",
    "Decisi√≥n de imputaci√≥n:\n",
    "\n",
    "Para Monto_Total se utiliz√≥ la mediana, porque es m√°s robusta ante valores extremos (outliers).\n",
    "Para Edad se utiliz√≥ la media, ya que es una variable num√©rica donde la imputaci√≥n es razonable para no perder registros.\n",
    "Outliers\n",
    "Se detectaron outliers en Monto_Total usando el m√©todo IQR (rango intercuart√≠lico).\n",
    "Se encontr√≥ 1 outlier principal (un monto extremadamente alto generado intencionalmente).\n",
    "\n",
    "Tratamiento aplicado: capping (recorte) al l√≠mite superior del IQR, en lugar de eliminar el registro, para conservar informaci√≥n y evitar distorsi√≥n en estad√≠sticas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14bd1a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Valores nulos por columna:\n",
      "ID               0\n",
      "Nombre           0\n",
      "Edad             2\n",
      "Ciudad           0\n",
      "Total_Compras    0\n",
      "Monto_Total      1\n",
      "dtype: int64\n",
      "\n",
      "üö® Se detectaron 1 outliers en Monto_Total.\n",
      "             Nombre  Monto_Total\n",
      "21  Cliente_Extra_2      50000.0\n",
      "\n",
      "‚úÖ Lecci√≥n 4 completada: Datos limpios y sin outliers extremos.\n"
     ]
    }
   ],
   "source": [
    "# --- LECCI√ìN 4: MANEJO DE VALORES PERDIDOS Y OUTLIERS ---\n",
    "\n",
    "# 1. Identificar valores nulos\n",
    "print(\"üîç Valores nulos por columna:\")\n",
    "print(df_consolidado.isnull().sum())\n",
    "\n",
    "# 2. Gesti√≥n de nulos: Imputaci√≥n\n",
    "# Vamos a llenar los montos nulos con la mediana (es m√°s segura que el promedio)\n",
    "mediana_monto = df_consolidado['Monto_Total'].median()\n",
    "df_consolidado['Monto_Total'] = df_consolidado['Monto_Total'].fillna(mediana_monto)\n",
    "\n",
    "# Tambi√©n hay nulos en Edad (del archivo original), los llenamos con la media\n",
    "df_consolidado['Edad'] = df_consolidado['Edad'].fillna(df_consolidado['Edad'].mean())\n",
    "\n",
    "# 3. Detecci√≥n de Outliers usando IQR (Rango Intercuart√≠lico)\n",
    "Q1 = df_consolidado['Monto_Total'].quantile(0.25)\n",
    "Q3 = df_consolidado['Monto_Total'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "limite_inferior = Q1 - 1.5 * IQR\n",
    "limite_superior = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identificamos qui√©nes son los outliers\n",
    "outliers = df_consolidado[(df_consolidado['Monto_Total'] < limite_inferior) | (df_consolidado['Monto_Total'] > limite_superior)]\n",
    "print(f\"\\nüö® Se detectaron {len(outliers)} outliers en Monto_Total.\")\n",
    "print(outliers[['Nombre', 'Monto_Total']])\n",
    "\n",
    "# 4. Tratamiento de Outliers: Los limitamos al valor del l√≠mite superior (Capping)\n",
    "df_consolidado.loc[df_consolidado['Monto_Total'] > limite_superior, 'Monto_Total'] = limite_superior\n",
    "\n",
    "# 5. Guardar DataFrame limpio\n",
    "df_consolidado.to_csv('clientes_limpio.csv', index=False)\n",
    "print(\"\\n‚úÖ Lecci√≥n 4 completada: Datos limpios y sin outliers extremos.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51daf125",
   "metadata": {},
   "source": [
    "## Lecci√≥n 5 ‚Äî Data Wrangling (transformaci√≥n y enriquecimiento)\n",
    "En esta etapa se aplicaron t√©cnicas para dejar el dataset m√°s √∫til para an√°lisis:\n",
    "\n",
    "Eliminaci√≥n de duplicados: al combinar CSV y Excel se detectaron 10 registros duplicados (mismo contenido), que fueron eliminados para evitar doble conteo.\n",
    "Conversi√≥n de tipos: se aseguraron tipos correctos en columnas clave (por ejemplo, ID y Edad como enteros).\n",
    "Creaci√≥n de nuevas variables:\n",
    "Ticket_Promedio = Monto_Total / Total_Compras\n",
    "Categoria_Cliente (Regular / Premium) seg√∫n gasto total.\n",
    "Rango_Etario (Joven / Adulto / Senior) usando discretizaci√≥n con pd.cut.\n",
    "Nota de calidad de datos: el contacto ‚ÄúCarlos‚Äù tiene Total_Compras = 0, por lo que Ticket_Promedio resulta NaN (divisi√≥n por cero). Esta situaci√≥n refleja un caso real: cuando no hay compras, el ticket promedio no aplica y debe tratarse en an√°lisis posterior (por ejemplo, con reglas de negocio o reemplazo condicionado).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "704cfc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros eliminados por duplicidad: 10\n",
      "\n",
      "‚úÖ Lecci√≥n 5 completada: Datos transformados y enriquecidos.\n",
      "   Nombre  Ticket_Promedio Categoria_Cliente Rango_Etario\n",
      "0     Ana            500.0           Regular        Joven\n",
      "1    Luis            600.0           Regular       Adulto\n",
      "2  Carlos              NaN           Regular       Adulto\n",
      "3   Marta            600.0           Regular        Joven\n",
      "4   Jorge            400.0           Premium       Adulto\n"
     ]
    }
   ],
   "source": [
    "# --- LECCI√ìN 5: DATA WRANGLING ---\n",
    "\n",
    "# 1. Eliminar registros duplicados (si existieran por la uni√≥n de archivos)\n",
    "antes = len(df_consolidado)\n",
    "df_consolidado = df_consolidado.drop_duplicates()\n",
    "print(f\"Registros eliminados por duplicidad: {antes - len(df_consolidado)}\")\n",
    "\n",
    "# 2. Transformar tipos de datos\n",
    "# Aseguramos que ID y Edad sean enteros (a veces quedan como float por los nulos)\n",
    "df_consolidado['ID'] = df_consolidado['ID'].astype(int)\n",
    "df_consolidado['Edad'] = df_consolidado['Edad'].astype(int)\n",
    "\n",
    "# 3. Crear una nueva columna calculada: 'Ticket_Promedio'\n",
    "# Es el monto total dividido por el total de compras\n",
    "df_consolidado['Ticket_Promedio'] = df_consolidado['Monto_Total'] / df_consolidado['Total_Compras']\n",
    "\n",
    "# 4. Aplicar funciones personalizadas (lambda)\n",
    "# Vamos a categorizar a los clientes seg√∫n su gasto\n",
    "# Si gasta m√°s de 3000 es \"Premium\", si no \"Regular\"\n",
    "df_consolidado['Categoria_Cliente'] = df_consolidado['Monto_Total'].apply(lambda x: 'Premium' if x > 3000 else 'Regular')\n",
    "\n",
    "# 5. Normalizaci√≥n / Discretizaci√≥n (Requerimiento t√©cnico)\n",
    "# Vamos a crear rangos de edad (Discretizaci√≥n)\n",
    "bins = [0, 30, 50, 100]\n",
    "labels = ['Joven', 'Adulto', 'Senior']\n",
    "df_consolidado['Rango_Etario'] = pd.cut(df_consolidado['Edad'], bins=bins, labels=labels)\n",
    "\n",
    "# Guardar versi√≥n optimizada\n",
    "df_consolidado.to_csv('clientes_optimizado.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Lecci√≥n 5 completada: Datos transformados y enriquecidos.\")\n",
    "print(df_consolidado[['Nombre', 'Ticket_Promedio', 'Categoria_Cliente', 'Rango_Etario']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9d164",
   "metadata": {},
   "source": [
    "## Lecci√≥n 6 ‚Äî Agrupamiento, pivoteo y exportaci√≥n final\n",
    "Para preparar el dataset para an√°lisis y reportes:\n",
    "\n",
    "Se gener√≥ un resumen por ciudad con groupby() (m√©tricas como gasto promedio, total de compras y cantidad de clientes).\n",
    "Se construy√≥ una tabla pivote (pivot_table) para observar el gasto promedio cruzando Rango_Etario y Categoria_Cliente.\n",
    "Entregables generados\n",
    "Dataset final exportado en:\n",
    "dataset_final_ecommerce.csv\n",
    "dataset_final_ecommerce.xlsx\n",
    "Con esto se completa el flujo integral de preparaci√≥n de datos: generaci√≥n, integraci√≥n, limpieza, transformaci√≥n y estructuraci√≥n para an√°lisis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80db7f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Reporte por Ciudad:\n",
      "              Monto_Total  Total_Compras  Cantidad_Clientes\n",
      "Ciudad                                                     \n",
      "Bah√≠a Blanca  2950.000000            6.0                  1\n",
      "Buenos Aires  2500.000000            5.0                  1\n",
      "C√≥rdoba       1200.000000            2.0                  1\n",
      "La Plata      3200.000000            8.0                  1\n",
      "Mendoza       1800.000000            3.0                  1\n",
      "Neuqu√©n       3300.000000            7.0                  1\n",
      "Rosario          0.000000            0.0                  1\n",
      "Salta         4100.000000           10.0                  1\n",
      "Santa Fe      2100.000000            4.0                  1\n",
      "Santiago      2709.432565           60.0                 10\n",
      "Tucum√°n        400.000000            1.0                  1\n",
      "\n",
      "Tabla Pivote (Gasto promedio por Rango Etario y Categor√≠a):\n",
      "Categoria_Cliente      Premium      Regular\n",
      "Rango_Etario                               \n",
      "Joven              3689.333397  2387.343593\n",
      "Adulto             4012.690691  1095.538761\n",
      "Senior             4964.952017  1515.899187\n",
      "\n",
      "‚úÖ Lecci√≥n 6 completada: Reportes generados y archivos finales exportados.\n",
      "üìÅ Archivos listos: 'dataset_final_ecommerce.csv' y 'dataset_final_ecommerce.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# --- LECCI√ìN 6: AGRUPAMIENTO Y PIVOTEO DE DATOS ---\n",
    "\n",
    "# 1. Agrupamiento (groupby) para obtener m√©tricas por Ciudad\n",
    "# Calculamos el promedio de gasto y el total de compras por ciudad\n",
    "reporte_ciudad = df_consolidado.groupby('Ciudad').agg({\n",
    "    'Monto_Total': 'mean',\n",
    "    'Total_Compras': 'sum',\n",
    "    'ID': 'count'\n",
    "}).rename(columns={'ID': 'Cantidad_Clientes'})\n",
    "\n",
    "print(\"üìä Reporte por Ciudad:\")\n",
    "print(reporte_ciudad)\n",
    "\n",
    "# 2. Reestructuraci√≥n de datos: Pivot Table\n",
    "# Vamos a ver el Monto_Total promedio cruzando Rango_Etario y Categoria_Cliente\n",
    "pivot_reporte = df_consolidado.pivot_table(\n",
    "    values='Monto_Total', \n",
    "    index='Rango_Etario', \n",
    "    columns='Categoria_Cliente', \n",
    "    aggfunc='mean',\n",
    "    observed=True # Para evitar advertencias con categor√≠as vac√≠as\n",
    ")\n",
    "\n",
    "print(\"\\nTabla Pivote (Gasto promedio por Rango Etario y Categor√≠a):\")\n",
    "print(pivot_reporte)\n",
    "\n",
    "# 3. Exportaci√≥n Final (Requerimiento obligatorio)\n",
    "df_consolidado.to_csv('dataset_final_ecommerce.csv', index=False)\n",
    "df_consolidado.to_excel('dataset_final_ecommerce.xlsx', index=True)\n",
    "\n",
    "print(\"\\n‚úÖ Lecci√≥n 6 completada: Reportes generados y archivos finales exportados.\")\n",
    "print(\"üìÅ Archivos listos: 'dataset_final_ecommerce.csv' y 'dataset_final_ecommerce.xlsx'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e7e138",
   "metadata": {},
   "source": [
    "## Posibles mejoras (a futuro)\n",
    "Implementar validaciones adicionales (por ejemplo, evitar Total_Compras = 0 si el negocio lo exige, o manejarlo con una regla expl√≠cita).\n",
    "Registrar el pipeline en funciones reutilizables (modularizaci√≥n) y/o logging para trazabilidad.\n",
    "Usar fuentes web alternativas o APIs con autenticaci√≥n para evitar bloqueos 403.\n",
    "Agregar tests simples (por ejemplo, validar que no queden nulos en columnas cr√≠ticas luego de la limpieza).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
